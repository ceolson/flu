{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/software/conda/envs/deepchem/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import deepchem as dc\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python import debug as tf_debug\n",
    "from deepchem.utils.genomics import encode_fasta_sequence\n",
    "from Bio import SeqIO\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.9\n",
    "config.gpu_options.allow_growth = True\n",
    "config.log_device_placement = False\n",
    "\n",
    "sess = tf.Session(config=config)\n",
    "tf.keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "x_train_flat = x_train.reshape([len(x_train),-1])\n",
    "\n",
    "max_size = 28*28\n",
    "encode_length = 1\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 784)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(x_train_flat[:batch_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_block(filter_name1,bias_name1,filter_name2,bias_name2,model,in_dim,out_dim,in_tensor):\n",
    "    with tf.variable_scope('',reuse=tf.AUTO_REUSE):\n",
    "        filter1 = tf.get_variable(filter_name1,collections=[model],trainable=True,shape=[5,in_dim,64])\n",
    "        bias1 = tf.get_variable(bias_name1,collections=[model],trainable=True,shape=[max_size,64])\n",
    "        filter2 = tf.get_variable(filter_name2,collections=[model],trainable=True,shape=[5,64,out_dim])\n",
    "        bias2 = tf.get_variable(bias_name2,collections=[model],trainable=True,shape=[max_size,out_dim])\n",
    "\n",
    "        x = in_tensor\n",
    "        x = tf.nn.leaky_relu(x)\n",
    "        \n",
    "        x = tf.nn.conv1d(x,filters=filter1,padding='SAME',stride=1)\n",
    "        x = tf.add(x,bias1)\n",
    "            \n",
    "        x = tf.nn.leaky_relu(x)\n",
    "        x = tf.nn.conv1d(x,filters=filter2,padding='SAME',stride=1)\n",
    "        x = tf.add(x,bias2)\n",
    "\n",
    "    return x+0.3*in_tensor\n",
    "    \n",
    "def dense(matrix,bias,model,in_dim,out_dim,in_tensor):\n",
    "    with tf.variable_scope('',reuse=tf.AUTO_REUSE):\n",
    "        W = tf.get_variable(matrix,collections=[model],trainable=True,shape=[in_dim,out_dim])\n",
    "        b = tf.get_variable(bias,collections=[model],trainable=True,shape=[out_dim,])\n",
    "\n",
    "    return tf.matmul(in_tensor,W) + b\n",
    "\n",
    "\n",
    "def conv(filter_name,bias_name,model,filter_shape,in_tensor):\n",
    "    with tf.variable_scope('',reuse=tf.AUTO_REUSE):\n",
    "        filt = tf.get_variable(filter_name,collections=[model],trainable=True,shape=filter_shape)\n",
    "        bias = tf.get_variable(bias_name,collections=[model],trainable=True,shape=[max_size,filter_shape[-1]])\n",
    "        \n",
    "    out = tf.nn.conv1d(in_tensor,filters=filt,padding='SAME',stride=1)\n",
    "    out = tf.add(out,bias)\n",
    "    return out\n",
    "    \n",
    "def batchnorm(sequence,offset_name,scale_name,model):\n",
    "\twith tf.variable_scope('',reuse=tf.AUTO_REUSE):\n",
    "\t\toffset = tf.get_variable(offset_name,collections=[model],trainable=True,initializer=tf.zeros(tf.shape(sequence)[1:]))\n",
    "\t\tscale = tf.get_variable(scale_name,collections=[model],trainable=True,initializer=tf.ones(tf.shape(sequence)[1:]))\n",
    "\t\n",
    "\tmeans,variances = tf.nn.moments(sequence,axes=[0])\n",
    "\tnormalized = tf.nn.batch_normalization(sequence,means,variances,offset,scale,tf.constant(0.0001))\n",
    "\t\n",
    "\treturn normalized\n",
    "    \n",
    "# Generator\n",
    "\n",
    "def generator(seed,training=True):\n",
    "    seed = tf.reshape(seed,(batch_size,100),name='generator.reshape1')\n",
    "    \n",
    "    seed2 = dense('generator.dense1.matrix','generator.dense1.bias','generator',100,max_size*64,seed)\n",
    "    seed2 = tf.nn.leaky_relu(seed2)\n",
    "    seed2 = tf.reshape(seed2,[batch_size,max_size,64])\n",
    "    \n",
    "    x = residual_block('generator.res1.filter1','generator.res1.bias1','generator.res1.filter2','generator.res1.bias2','generator',64,64,seed2)\n",
    "    if training: x = batchnorm(x,'generator.batchnorm1.offset','generator.batchnorm1.scale','generator')\n",
    "    \n",
    "    x = residual_block('generator.res2.filter1','generator.res2.bias1','generator.res2.filter2','generator.res2.bias2','generator',64,64,x)\n",
    "    if training: x = batchnorm(x,'generator.batchnorm2.offset','generator.batchnorm2.scale','generator')\n",
    "    \n",
    "    x = residual_block('generator.res3.filter1','generator.res3.bias1','generator.res3.filter2','generator.res3.bias2','generator',64,64,x)\n",
    "    if training: x = batchnorm(x,'generator.batchnorm3.offset','generator.batchnorm3.scale','generator')\n",
    "    \n",
    "    x = residual_block('generator.res4.filter1','generator.res4.bias1','generator.res4.filter2','generator.res4.bias2','generator',64,64,x)\n",
    "    if training: x = batchnorm(x,'generator.batchnorm4.offset','generator.batchnorm4.scale','generator')\n",
    "    \n",
    "    x = residual_block('generator.res5.filter1','generator.res5.bias1','generator.res5.filter2','generator.res5.bias2','generator',64,64,x)\n",
    "    if training: x = batchnorm(x,'generator.batchnorm5.offset','generator.batchnorm5.scale','generator')\n",
    "\n",
    "    x = conv('generator.conv1.filter','generator.conv1.bias','generator',(5,64,encode_length),x)\n",
    "    \n",
    "    synthetic = tf.reshape(x,[batch_size,max_size],name='generator.reshape2')\n",
    "    return synthetic\n",
    "\n",
    "# Discriminator\n",
    "\n",
    "def discriminator(sequence):\n",
    "    sequence = tf.reshape(sequence,[batch_size,max_size,encode_length],name='discriminator.reshape1')\n",
    "    x = conv('discriminator.conv1.filter','discriminator.conv1.bias','discriminator',(5,encode_length,64),sequence)\n",
    "    x = tf.nn.leaky_relu(x)\n",
    "    \n",
    "    x = residual_block('discriminator.res1.filter1','discriminator.res1.bias1','discriminator.res1.filter2','discriminator.res1.bias1','discriminator',64,64,x)\n",
    "    x = residual_block('discriminator.res2.filter1','discriminator.res2.bias1','discriminator.res2.filter2','discriminator.res2.bias1','discriminator',64,64,x)\n",
    "    x = residual_block('discriminator.res3.filter1','discriminator.res3.bias1','discriminator.res3.filter2','discriminator.res3.bias1','discriminator',64,64,x)\n",
    "    x = residual_block('discriminator.res4.filter1','discriminator.res4.bias1','discriminator.res4.filter2','discriminator.res4.bias1','discriminator',64,64,x)\n",
    "    x = residual_block('discriminator.res5.filter1','discriminator.res5.bias1','discriminator.res5.filter2','discriminator.res5.bias1','discriminator',64,64,x)\n",
    "    \n",
    "    x = tf.reshape(x,(batch_size,max_size*64),name='discriminator.reshape2')\n",
    "    \n",
    "    output = dense('discriminator.dense1.matrix','discriminator.dense1.bias','discriminator',max_size*64,1,x)\n",
    "    return output\n",
    "    \n",
    "### Constructing the loss function\n",
    "\n",
    "real_images = tf.placeholder(shape=[batch_size,max_size],dtype='float32',name='real_images')\n",
    "noise = tf.placeholder(shape=[batch_size,100],dtype='float32',name='noise')\n",
    "\n",
    "fake_images = generator(noise)\n",
    "fake_images = tf.identity(fake_images,name='fake_images')\n",
    "\n",
    "# Sampling images in the encoded space between the fake ones and the real ones\n",
    "\n",
    "interpolation_coeffs = tf.random_uniform(shape=(batch_size,1))\n",
    "sampled_images = tf.add(real_images,tf.multiply(tf.subtract(fake_images,real_images),interpolation_coeffs),name='sampled_images')\n",
    "\n",
    "# Gradient penalty\n",
    "gradients = tf.gradients(discriminator(sampled_images),sampled_images,name='gradients')[0]\n",
    "norms = tf.norm(gradients,axis=1)\n",
    "score = tf.reduce_mean(tf.square(tf.subtract(norms,1.)),name='gradient_penalty')\n",
    "\n",
    "# Loss based on discriminator's predictions\n",
    "\n",
    "pred_real = tf.reshape(discriminator(real_images),[-1],name='reshape.a')\n",
    "pred_real = tf.identity(pred_real,name='pred_real')\n",
    "\n",
    "pred_fake = tf.reshape(discriminator(fake_images),[-1],name='reshape.b')\n",
    "pred_fake = tf.identity(pred_fake,name='pred_fake')\n",
    "\n",
    "diff = tf.reduce_mean(tf.subtract(pred_fake,pred_real))\n",
    "\n",
    "# Discriminator wants fake sequences to be labeled 0, real to be labeled 1\n",
    "disc_loss = tf.add(diff,tf.multiply(tf.constant(10.),score),name='disc_loss')\n",
    "\n",
    "# Generator wants fake sequences to be labeled 1\n",
    "gen_loss = - tf.reduce_mean(pred_fake,name='gen_loss')\n",
    "\n",
    "# For tracking using tensorboard\n",
    "\n",
    "a = tf.summary.scalar('discriminator_difference', diff)\n",
    "b = tf.summary.scalar('generator_difference',gen_loss)\n",
    "merged = tf.summary.merge_all()\n",
    "\n",
    "# Optimizers\n",
    "disc_optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.0001)\n",
    "gen_optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.0001)\n",
    "\n",
    "train_discriminator = disc_optimizer.minimize(disc_loss,var_list=tf.get_collection('discriminator'),name='train_discriminator')\n",
    "grads_discriminator = disc_optimizer.compute_gradients(disc_loss,var_list=tf.get_collection('discriminator'))\n",
    "\n",
    "train_generator = gen_optimizer.minimize(gen_loss,var_list=tf.get_collection('generator'),name='train_generator')\n",
    "grads_generator = gen_optimizer.compute_gradients(gen_loss,var_list=tf.get_collection('generator'))\n",
    "\n",
    "init = tf.initializers.variables(tf.get_collection('discriminator')+tf.get_collection('generator'))\n",
    "\n",
    "writer = tf.summary.FileWriter('/home/ceolson0/Documents/tensorboard',sess.graph)\n",
    "saver = tf.train.Saver(tf.get_collection('discriminator')+tf.get_collection('generator'))\n",
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############\n",
      "6331793\n"
     ]
    }
   ],
   "source": [
    "sess.run(init)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "print('############')\n",
    "print(np.sum([np.prod(v.get_shape().as_list()) for v in tf.trainable_variables()]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "gan17, epoch  0\n",
      "Generator loss:  1.1679552\n",
      "Discriminator loss:  -0.0044605946\n",
      "\n",
      "gan17, epoch  1\n",
      "Generator loss:  1.1683056\n",
      "Discriminator loss:  -0.0048128366\n",
      "\n",
      "gan17, epoch  2\n",
      "Generator loss:  1.1686541\n",
      "Discriminator loss:  -0.005242512\n",
      "\n",
      "gan17, epoch  3\n",
      "Generator loss:  1.1689895\n",
      "Discriminator loss:  -0.005106319\n",
      "\n",
      "gan17, epoch  4\n",
      "Generator loss:  1.1693397\n",
      "Discriminator loss:  -0.005589882\n",
      "\n",
      "gan17, epoch  5\n",
      "Generator loss:  1.1696727\n",
      "Discriminator loss:  -0.0054268586\n",
      "\n",
      "gan17, epoch  6\n",
      "Generator loss:  1.1700342\n",
      "Discriminator loss:  -0.0068038036\n",
      "\n",
      "gan17, epoch  7\n",
      "Generator loss:  1.1703968\n",
      "Discriminator loss:  -0.007256368\n",
      "\n",
      "gan17, epoch  8\n",
      "Generator loss:  1.1707577\n",
      "Discriminator loss:  -0.007965089\n",
      "\n",
      "gan17, epoch  9\n",
      "Generator loss:  1.171104\n",
      "Discriminator loss:  -0.0074549005\n",
      "\n",
      "gan17, epoch  10\n",
      "Generator loss:  1.1714469\n",
      "Discriminator loss:  -0.007376504\n",
      "\n",
      "gan17, epoch  11\n",
      "Generator loss:  1.1718217\n",
      "Discriminator loss:  -0.008206959\n",
      "\n",
      "gan17, epoch  12\n",
      "Generator loss:  1.1721848\n",
      "Discriminator loss:  -0.0078100385\n",
      "\n",
      "gan17, epoch  13\n",
      "Generator loss:  1.1725683\n",
      "Discriminator loss:  -0.008687474\n",
      "\n",
      "gan17, epoch  14\n",
      "Generator loss:  1.1729074\n",
      "Discriminator loss:  -0.008655094\n",
      "\n",
      "gan17, epoch  15\n",
      "Generator loss:  1.1732814\n",
      "Discriminator loss:  -0.009543726\n",
      "\n",
      "gan17, epoch  16\n",
      "Generator loss:  1.1736547\n",
      "Discriminator loss:  -0.0091061\n",
      "\n",
      "gan17, epoch  17\n",
      "Generator loss:  1.1740324\n",
      "Discriminator loss:  -0.010006075\n",
      "\n",
      "gan17, epoch  18\n",
      "Generator loss:  1.174385\n",
      "Discriminator loss:  -0.010982871\n",
      "\n",
      "gan17, epoch  19\n",
      "Generator loss:  1.1747649\n",
      "Discriminator loss:  -0.010209896\n",
      "\n",
      "gan17, epoch  20\n",
      "Generator loss:  1.1751329\n",
      "Discriminator loss:  -0.010370909\n",
      "\n",
      "gan17, epoch  21\n",
      "Generator loss:  1.1755108\n",
      "Discriminator loss:  -0.0114206625\n",
      "\n",
      "gan17, epoch  22\n",
      "Generator loss:  1.1759056\n",
      "Discriminator loss:  -0.011097353\n",
      "\n",
      "gan17, epoch  23\n",
      "Generator loss:  1.1762702\n",
      "Discriminator loss:  -0.012039666\n",
      "\n",
      "gan17, epoch  24\n",
      "Generator loss:  1.176628\n",
      "Discriminator loss:  -0.01248013\n",
      "\n",
      "gan17, epoch  25\n",
      "Generator loss:  1.1770204\n",
      "Discriminator loss:  -0.013195799\n",
      "\n",
      "gan17, epoch  26\n",
      "Generator loss:  1.1774001\n",
      "Discriminator loss:  -0.012211282\n",
      "\n",
      "gan17, epoch  27\n",
      "Generator loss:  1.1777853\n",
      "Discriminator loss:  -0.01266184\n",
      "\n",
      "gan17, epoch  28\n",
      "Generator loss:  1.1781585\n",
      "Discriminator loss:  -0.013320728\n",
      "\n",
      "gan17, epoch  29\n",
      "Generator loss:  1.1785668\n",
      "Discriminator loss:  -0.013473634\n",
      "\n",
      "gan17, epoch  30\n",
      "Generator loss:  1.1789336\n",
      "Discriminator loss:  -0.013856602\n",
      "\n",
      "gan17, epoch  31\n",
      "Generator loss:  1.1793122\n",
      "Discriminator loss:  -0.013040782\n",
      "\n",
      "gan17, epoch  32\n",
      "Generator loss:  1.1797221\n",
      "Discriminator loss:  -0.014404948\n",
      "\n",
      "gan17, epoch  33\n",
      "Generator loss:  1.1801153\n",
      "Discriminator loss:  -0.014883694\n",
      "\n",
      "gan17, epoch  34\n",
      "Generator loss:  1.1805012\n",
      "Discriminator loss:  -0.015654033\n",
      "\n",
      "gan17, epoch  35\n",
      "Generator loss:  1.1808822\n",
      "Discriminator loss:  -0.014704984\n",
      "\n",
      "gan17, epoch  36\n",
      "Generator loss:  1.1813135\n",
      "Discriminator loss:  -0.015906336\n",
      "\n",
      "gan17, epoch  37\n",
      "Generator loss:  1.1816833\n",
      "Discriminator loss:  -0.016215578\n",
      "\n",
      "gan17, epoch  38\n",
      "Generator loss:  1.1821164\n",
      "Discriminator loss:  -0.016801903\n",
      "\n",
      "gan17, epoch  39\n",
      "Generator loss:  1.1824884\n",
      "Discriminator loss:  -0.017066712\n",
      "\n",
      "gan17, epoch  40\n",
      "Generator loss:  1.18291\n",
      "Discriminator loss:  -0.01680708\n",
      "\n",
      "gan17, epoch  41\n",
      "Generator loss:  1.1833179\n",
      "Discriminator loss:  -0.017006868\n",
      "\n",
      "gan17, epoch  42\n",
      "Generator loss:  1.1836891\n",
      "Discriminator loss:  -0.016694762\n",
      "\n",
      "gan17, epoch  43\n",
      "Generator loss:  1.1841148\n",
      "Discriminator loss:  -0.018646987\n",
      "\n",
      "gan17, epoch  44\n",
      "Generator loss:  1.1844985\n",
      "Discriminator loss:  -0.017781548\n",
      "\n",
      "gan17, epoch  45\n",
      "Generator loss:  1.184916\n",
      "Discriminator loss:  -0.018728416\n",
      "\n",
      "gan17, epoch  46\n",
      "Generator loss:  1.1853282\n",
      "Discriminator loss:  -0.019323818\n",
      "\n",
      "gan17, epoch  47\n",
      "Generator loss:  1.1857477\n",
      "Discriminator loss:  -0.019655392\n",
      "\n",
      "gan17, epoch  48\n",
      "Generator loss:  1.1861633\n",
      "Discriminator loss:  -0.019386383\n",
      "\n",
      "gan17, epoch  49\n",
      "Generator loss:  1.1865834\n",
      "Discriminator loss:  -0.020099169\n",
      "\n",
      "gan17, epoch  50\n",
      "Generator loss:  1.1869769\n",
      "Discriminator loss:  -0.020057207\n",
      "\n",
      "gan17, epoch  51\n",
      "Generator loss:  1.1874067\n",
      "Discriminator loss:  -0.020808835\n",
      "\n",
      "gan17, epoch  52\n",
      "Generator loss:  1.1878451\n",
      "Discriminator loss:  -0.02166061\n",
      "\n",
      "gan17, epoch  53\n",
      "Generator loss:  1.1882446\n",
      "Discriminator loss:  -0.020985708\n",
      "\n",
      "gan17, epoch  54\n",
      "Generator loss:  1.1886455\n",
      "Discriminator loss:  -0.021763973\n",
      "\n",
      "gan17, epoch  55\n",
      "Generator loss:  1.1890903\n",
      "Discriminator loss:  -0.020969223\n",
      "\n",
      "gan17, epoch  56\n",
      "Generator loss:  1.1894988\n",
      "Discriminator loss:  -0.021833912\n",
      "\n",
      "gan17, epoch  57\n",
      "Generator loss:  1.1899358\n",
      "Discriminator loss:  -0.02358562\n",
      "\n",
      "gan17, epoch  58\n",
      "Generator loss:  1.1903707\n",
      "Discriminator loss:  -0.024265783\n",
      "\n",
      "gan17, epoch  59\n",
      "Generator loss:  1.1908056\n",
      "Discriminator loss:  -0.023019252\n",
      "\n",
      "gan17, epoch  60\n",
      "Generator loss:  1.1912293\n",
      "Discriminator loss:  -0.024844717\n",
      "\n",
      "gan17, epoch  61\n",
      "Generator loss:  1.1916391\n",
      "Discriminator loss:  -0.024465794\n",
      "\n",
      "gan17, epoch  62\n",
      "Generator loss:  1.1920879\n",
      "Discriminator loss:  -0.02400654\n",
      "\n",
      "gan17, epoch  63\n",
      "Generator loss:  1.192513\n",
      "Discriminator loss:  -0.025545988\n",
      "\n",
      "gan17, epoch  64\n",
      "Generator loss:  1.1929772\n",
      "Discriminator loss:  -0.025467573\n",
      "\n",
      "gan17, epoch  65\n",
      "Generator loss:  1.1934168\n",
      "Discriminator loss:  -0.027006935\n",
      "\n",
      "gan17, epoch  66\n",
      "Generator loss:  1.1938565\n",
      "Discriminator loss:  -0.026275892\n",
      "\n",
      "gan17, epoch  67\n",
      "Generator loss:  1.1943058\n",
      "Discriminator loss:  -0.02566971\n",
      "\n",
      "gan17, epoch  68\n",
      "Generator loss:  1.1947303\n",
      "Discriminator loss:  -0.028577311\n",
      "\n",
      "gan17, epoch  69\n",
      "Generator loss:  1.1951826\n",
      "Discriminator loss:  -0.027194133\n",
      "\n",
      "gan17, epoch  70\n",
      "Generator loss:  1.1956455\n",
      "Discriminator loss:  -0.027685048\n",
      "\n",
      "gan17, epoch  71\n",
      "Generator loss:  1.1960837\n",
      "Discriminator loss:  -0.02755188\n",
      "\n",
      "gan17, epoch  72\n",
      "Generator loss:  1.1965337\n",
      "Discriminator loss:  -0.029113011\n",
      "\n",
      "gan17, epoch  73\n",
      "Generator loss:  1.196983\n",
      "Discriminator loss:  -0.027729375\n",
      "\n",
      "gan17, epoch  74\n",
      "Generator loss:  1.1974579\n",
      "Discriminator loss:  -0.029445838\n",
      "\n",
      "gan17, epoch  75\n",
      "Generator loss:  1.1978829\n",
      "Discriminator loss:  -0.029616307\n",
      "\n",
      "gan17, epoch  76\n",
      "Generator loss:  1.1983646\n",
      "Discriminator loss:  -0.029331733\n",
      "\n",
      "gan17, epoch  77\n",
      "Generator loss:  1.1988313\n",
      "Discriminator loss:  -0.031859275\n",
      "\n",
      "gan17, epoch  78\n",
      "Generator loss:  1.1992838\n",
      "Discriminator loss:  -0.030319136\n",
      "\n",
      "gan17, epoch  79\n",
      "Generator loss:  1.1997637\n",
      "Discriminator loss:  -0.031673327\n",
      "\n",
      "gan17, epoch  80\n",
      "Generator loss:  1.200232\n",
      "Discriminator loss:  -0.031981062\n",
      "\n",
      "gan17, epoch  81\n",
      "Generator loss:  1.2006692\n",
      "Discriminator loss:  -0.033485413\n",
      "\n",
      "gan17, epoch  82\n",
      "Generator loss:  1.2011572\n",
      "Discriminator loss:  -0.03264183\n",
      "\n",
      "gan17, epoch  83\n",
      "Generator loss:  1.2016338\n",
      "Discriminator loss:  -0.033170875\n",
      "\n",
      "gan17, epoch  84\n",
      "Generator loss:  1.2020992\n",
      "Discriminator loss:  -0.03281348\n",
      "\n",
      "gan17, epoch  85\n",
      "Generator loss:  1.2025574\n",
      "Discriminator loss:  -0.033880904\n",
      "\n",
      "gan17, epoch  86\n",
      "Generator loss:  1.2030511\n",
      "Discriminator loss:  -0.03444833\n",
      "\n",
      "gan17, epoch  87\n",
      "Generator loss:  1.2035162\n",
      "Discriminator loss:  -0.03550731\n",
      "\n",
      "gan17, epoch  88\n",
      "Generator loss:  1.2039968\n",
      "Discriminator loss:  -0.03321094\n",
      "\n",
      "gan17, epoch  89\n",
      "Generator loss:  1.2044908\n",
      "Discriminator loss:  -0.034586597\n",
      "\n",
      "gan17, epoch  90\n",
      "Generator loss:  1.2049385\n",
      "Discriminator loss:  -0.034625772\n",
      "\n",
      "gan17, epoch  91\n",
      "Generator loss:  1.2054406\n",
      "Discriminator loss:  -0.03663816\n",
      "\n",
      "gan17, epoch  92\n",
      "Generator loss:  1.2059346\n",
      "Discriminator loss:  -0.0366853\n",
      "\n",
      "gan17, epoch  93\n",
      "Generator loss:  1.2064232\n",
      "Discriminator loss:  -0.038367063\n",
      "\n",
      "gan17, epoch  94\n",
      "Generator loss:  1.2069097\n",
      "Discriminator loss:  -0.03788041\n",
      "\n",
      "gan17, epoch  95\n",
      "Generator loss:  1.2073847\n",
      "Discriminator loss:  -0.03799903\n",
      "\n",
      "gan17, epoch  96\n",
      "Generator loss:  1.2078881\n",
      "Discriminator loss:  -0.03851919\n",
      "\n",
      "gan17, epoch  97\n",
      "Generator loss:  1.2083851\n",
      "Discriminator loss:  -0.03798238\n",
      "\n",
      "gan17, epoch  98\n",
      "Generator loss:  1.2089057\n",
      "Discriminator loss:  -0.039845575\n",
      "\n",
      "gan17, epoch  99\n",
      "Generator loss:  1.2093908\n",
      "Discriminator loss:  -0.040097937\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    print('\\ngan17, epoch ',epoch)\n",
    "\n",
    "    \n",
    "    # Train discriminator\n",
    "    for i in range(5):\n",
    "        real = np.random.permutation(x_train_flat)[:batch_size].astype(np.float32)\n",
    "        noise_input = np.random.normal(0,1,(batch_size,100))\n",
    "        _,d_loss,grads = sess.run([train_discriminator,diff,grads_discriminator],feed_dict={real_images:real,noise:noise_input})\n",
    "            \n",
    "    # Train generator\n",
    "\n",
    "    real = np.random.permutation(x_train_flat)[:batch_size].astype(np.float32)\n",
    "    noise_input = np.random.normal(0,1,(batch_size,100))\n",
    "    _,g_loss,grads = sess.run([train_generator,gen_loss,grads_generator],feed_dict={noise:noise_input})\n",
    "\n",
    "    print(\"Generator loss: \",g_loss)\n",
    "    print(\"Discriminator loss: \",d_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/ceolson0/Documents/gan_mnist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/ceolson0/Documents/gan_mnist\n"
     ]
    }
   ],
   "source": [
    "saver.restore(sess,'/home/ceolson0/Documents/gan_mnist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.57356148  0.34128067  0.28313271 -1.0815925   0.86436929 -0.56313264\n",
      "  1.37660261  1.32725203  1.06678549 -0.18994896  0.62321006  0.59150533\n",
      " -0.40708093  0.88236717 -2.40676889 -0.6613818  -1.14748409 -1.34204855\n",
      " -1.96332638  1.46406398 -0.08875044 -1.43010537 -0.96564181 -0.54305249\n",
      "  2.93270942 -0.78768378 -0.65036777 -0.13342682  1.37795027 -0.24563945\n",
      "  0.15371334  0.68842117 -0.00469619 -0.50251136 -1.52345731  0.15933675\n",
      " -0.13033443 -2.52473924  1.02594581  0.37207823  0.19797066  0.3693612\n",
      " -1.80758171 -0.04119154  0.22908267 -0.80847861  1.21469856  1.02790408\n",
      "  0.6168185  -2.16445786  0.17150884  1.41838121  0.46454166 -2.86899617\n",
      " -0.11482432 -2.26361238  0.69599094  0.66933951  1.30633635 -1.15753108\n",
      " -0.49755856 -0.31507061 -0.68800134  0.08447894 -1.49370931  0.22069852\n",
      " -2.45553446  0.59922731 -0.16454428 -1.53141423  0.51131707 -1.7158955\n",
      "  0.3223182  -1.25670978 -0.89335974  1.56058094  0.81902913 -2.34115837\n",
      " -1.70963808  1.53297736 -1.83739531 -1.71280943 -0.14685372  0.26752404\n",
      "  1.41965218 -0.76685802 -0.95351905  0.49901269 -0.26860435 -0.68768214\n",
      "  0.6500005  -0.0451049  -1.11276428  0.27141051  0.34233818  1.1294628\n",
      " -0.80491629  2.78257323  0.41145154  2.33646715]\n"
     ]
    }
   ],
   "source": [
    "test_noise = np.random.normal(0,1,(100,100))\n",
    "print(test_noise[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = sess.run(generator(tf.random_normal((batch_size,100)),training=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7ffe5885e8d0>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "im = test_image[0].reshape((28,28))\n",
    "#im = np.stack([im,im,im],axis=-1)\n",
    "from matplotlib import pyplot as plt\n",
    "plt.imshow(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepChem (Python 3.7)",
   "language": "python",
   "name": "deepchem"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
