{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/software/conda/envs/deepchem/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import deepchem as dc\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python import debug as tf_debug\n",
    "from deepchem.utils.genomics import encode_fasta_sequence\n",
    "from Bio import SeqIO\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.9\n",
    "config.gpu_options.allow_growth = True\n",
    "config.log_device_placement = False\n",
    "\n",
    "sess = tf.Session(config=config)\n",
    "tf.keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "x_train_flat = x_train.reshape([len(x_train),-1])\n",
    "\n",
    "max_size = 28*28\n",
    "encode_length = 1\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 784)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(x_train_flat[:batch_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_block(filter_name1,bias_name1,filter_name2,bias_name2,model,in_dim,out_dim,in_tensor):\n",
    "    with tf.variable_scope('',reuse=tf.AUTO_REUSE):\n",
    "        filter1 = tf.get_variable(filter_name1,collections=[model],trainable=True,shape=[5,in_dim,64])\n",
    "        bias1 = tf.get_variable(bias_name1,collections=[model],trainable=True,shape=[max_size,64])\n",
    "        filter2 = tf.get_variable(filter_name2,collections=[model],trainable=True,shape=[5,64,out_dim])\n",
    "        bias2 = tf.get_variable(bias_name2,collections=[model],trainable=True,shape=[max_size,out_dim])\n",
    "\n",
    "        x = in_tensor\n",
    "        x = tf.nn.leaky_relu(x)\n",
    "        \n",
    "        x = tf.nn.conv1d(x,filters=filter1,padding='SAME',stride=1)\n",
    "        x = tf.add(x,bias1)\n",
    "            \n",
    "        x = tf.nn.leaky_relu(x)\n",
    "        x = tf.nn.conv1d(x,filters=filter2,padding='SAME',stride=1)\n",
    "        x = tf.add(x,bias2)\n",
    "\n",
    "    return x+0.3*in_tensor\n",
    "    \n",
    "def dense(matrix,bias,model,in_dim,out_dim,in_tensor):\n",
    "    with tf.variable_scope('',reuse=tf.AUTO_REUSE):\n",
    "        W = tf.get_variable(matrix,collections=[model],trainable=True,shape=[in_dim,out_dim])\n",
    "        b = tf.get_variable(bias,collections=[model],trainable=True,shape=[out_dim,])\n",
    "\n",
    "    return tf.matmul(in_tensor,W) + b\n",
    "\n",
    "\n",
    "def conv(filter_name,bias_name,model,filter_shape,in_tensor):\n",
    "    with tf.variable_scope('',reuse=tf.AUTO_REUSE):\n",
    "        filt = tf.get_variable(filter_name,collections=[model],trainable=True,shape=filter_shape)\n",
    "        bias = tf.get_variable(bias_name,collections=[model],trainable=True,shape=[max_size,filter_shape[-1]])\n",
    "        \n",
    "    out = tf.nn.conv1d(in_tensor,filters=filt,padding='SAME',stride=1)\n",
    "    out = tf.add(out,bias)\n",
    "    return out\n",
    "    \n",
    "def batchnorm(sequence,offset_name,scale_name,model):\n",
    "\twith tf.variable_scope('',reuse=tf.AUTO_REUSE):\n",
    "\t\toffset = tf.get_variable(offset_name,collections=[model],trainable=True,initializer=tf.zeros(tf.shape(sequence)[1:]))\n",
    "\t\tscale = tf.get_variable(scale_name,collections=[model],trainable=True,initializer=tf.ones(tf.shape(sequence)[1:]))\n",
    "\t\n",
    "\tmeans,variances = tf.nn.moments(sequence,axes=[0])\n",
    "\tnormalized = tf.nn.batch_normalization(sequence,means,variances,offset,scale,tf.constant(0.0001))\n",
    "\t\n",
    "\treturn normalized\n",
    "    \n",
    "# Generator\n",
    "\n",
    "def generator(seed,training=True):\n",
    "    seed = tf.reshape(seed,(batch_size,100),name='generator.reshape1')\n",
    "    \n",
    "    seed2 = dense('generator.dense1.matrix','generator.dense1.bias','generator',100,max_size*64,seed)\n",
    "    seed2 = tf.nn.leaky_relu(seed2)\n",
    "    seed2 = tf.reshape(seed2,[batch_size,max_size,64])\n",
    "    \n",
    "    x = residual_block('generator.res1.filter1','generator.res1.bias1','generator.res1.filter2','generator.res1.bias2','generator',64,64,seed2)\n",
    "    if training: x = batchnorm(x,'generator.batchnorm1.offset','generator.batchnorm1.scale','generator')\n",
    "    \n",
    "    x = residual_block('generator.res2.filter1','generator.res2.bias1','generator.res2.filter2','generator.res2.bias2','generator',64,64,x)\n",
    "    if training: x = batchnorm(x,'generator.batchnorm2.offset','generator.batchnorm2.scale','generator')\n",
    "    \n",
    "    x = residual_block('generator.res3.filter1','generator.res3.bias1','generator.res3.filter2','generator.res3.bias2','generator',64,64,x)\n",
    "    if training: x = batchnorm(x,'generator.batchnorm3.offset','generator.batchnorm3.scale','generator')\n",
    "    \n",
    "    x = residual_block('generator.res4.filter1','generator.res4.bias1','generator.res4.filter2','generator.res4.bias2','generator',64,64,x)\n",
    "    if training: x = batchnorm(x,'generator.batchnorm4.offset','generator.batchnorm4.scale','generator')\n",
    "    \n",
    "    x = residual_block('generator.res5.filter1','generator.res5.bias1','generator.res5.filter2','generator.res5.bias2','generator',64,64,x)\n",
    "    if training: x = batchnorm(x,'generator.batchnorm5.offset','generator.batchnorm5.scale','generator')\n",
    "\n",
    "    x = conv('generator.conv1.filter','generator.conv1.bias','generator',(5,64,encode_length),x)\n",
    "    \n",
    "    synthetic = tf.reshape(x,[batch_size,max_size],name='generator.reshape2')\n",
    "    return synthetic\n",
    "\n",
    "# Discriminator\n",
    "\n",
    "def discriminator(sequence):\n",
    "    sequence = tf.reshape(sequence,[batch_size,max_size,encode_length],name='discriminator.reshape1')\n",
    "    x = conv('discriminator.conv1.filter','discriminator.conv1.bias','discriminator',(5,encode_length,64),sequence)\n",
    "    x = tf.nn.leaky_relu(x)\n",
    "    \n",
    "    x = residual_block('discriminator.res1.filter1','discriminator.res1.bias1','discriminator.res1.filter2','discriminator.res1.bias1','discriminator',64,64,x)\n",
    "    x = residual_block('discriminator.res2.filter1','discriminator.res2.bias1','discriminator.res2.filter2','discriminator.res2.bias1','discriminator',64,64,x)\n",
    "    x = residual_block('discriminator.res3.filter1','discriminator.res3.bias1','discriminator.res3.filter2','discriminator.res3.bias1','discriminator',64,64,x)\n",
    "    x = residual_block('discriminator.res4.filter1','discriminator.res4.bias1','discriminator.res4.filter2','discriminator.res4.bias1','discriminator',64,64,x)\n",
    "    x = residual_block('discriminator.res5.filter1','discriminator.res5.bias1','discriminator.res5.filter2','discriminator.res5.bias1','discriminator',64,64,x)\n",
    "    \n",
    "    x = tf.reshape(x,(batch_size,max_size*64),name='discriminator.reshape2')\n",
    "    \n",
    "    output = dense('discriminator.dense1.matrix','discriminator.dense1.bias','discriminator',max_size*64,1,x)\n",
    "    return output\n",
    "    \n",
    "### Constructing the loss function\n",
    "\n",
    "real_images = tf.placeholder(shape=[batch_size,max_size],dtype='float32',name='real_images')\n",
    "noise = tf.placeholder(shape=[batch_size,100],dtype='float32',name='noise')\n",
    "\n",
    "fake_images = generator(noise)\n",
    "fake_images = tf.identity(fake_images,name='fake_images')\n",
    "\n",
    "# Sampling images in the encoded space between the fake ones and the real ones\n",
    "\n",
    "interpolation_coeffs = tf.random_uniform(shape=(batch_size,1))\n",
    "sampled_images = tf.add(real_images,tf.multiply(tf.subtract(fake_images,real_images),interpolation_coeffs),name='sampled_images')\n",
    "\n",
    "# Gradient penalty\n",
    "gradients = tf.gradients(discriminator(sampled_images),sampled_images,name='gradients')[0]\n",
    "norms = tf.norm(gradients,axis=1)\n",
    "score = tf.reduce_mean(tf.square(tf.subtract(norms,1.)),name='gradient_penalty')\n",
    "\n",
    "# Loss based on discriminator's predictions\n",
    "\n",
    "pred_real = tf.reshape(discriminator(real_images),[-1],name='reshape.a')\n",
    "pred_real = tf.identity(pred_real,name='pred_real')\n",
    "\n",
    "pred_fake = tf.reshape(discriminator(fake_images),[-1],name='reshape.b')\n",
    "pred_fake = tf.identity(pred_fake,name='pred_fake')\n",
    "\n",
    "diff = tf.reduce_mean(tf.subtract(pred_fake,pred_real))\n",
    "\n",
    "# Discriminator wants fake sequences to be labeled 0, real to be labeled 1\n",
    "disc_loss = tf.add(diff,tf.multiply(tf.constant(10.),score),name='disc_loss')\n",
    "\n",
    "# Generator wants fake sequences to be labeled 1\n",
    "gen_loss = - tf.reduce_mean(pred_fake,name='gen_loss')\n",
    "\n",
    "# For tracking using tensorboard\n",
    "\n",
    "a = tf.summary.scalar('discriminator_difference', diff)\n",
    "b = tf.summary.scalar('generator_difference',gen_loss)\n",
    "merged = tf.summary.merge_all()\n",
    "\n",
    "# Optimizers\n",
    "disc_optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.0001)\n",
    "gen_optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.0001)\n",
    "\n",
    "train_discriminator = disc_optimizer.minimize(disc_loss,var_list=tf.get_collection('discriminator'),name='train_discriminator')\n",
    "grads_discriminator = disc_optimizer.compute_gradients(disc_loss,var_list=tf.get_collection('discriminator'))\n",
    "\n",
    "train_generator = gen_optimizer.minimize(gen_loss,var_list=tf.get_collection('generator'),name='train_generator')\n",
    "grads_generator = gen_optimizer.compute_gradients(gen_loss,var_list=tf.get_collection('generator'))\n",
    "\n",
    "init = tf.initializers.variables(tf.get_collection('discriminator')+tf.get_collection('generator'))\n",
    "\n",
    "writer = tf.summary.FileWriter('/home/ceolson0/Documents/tensorboard',sess.graph)\n",
    "saver = tf.train.Saver(tf.get_collection('discriminator')+tf.get_collection('generator'))\n",
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############\n",
      "6331793\n"
     ]
    }
   ],
   "source": [
    "sess.run(init)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "print('############')\n",
    "print(np.sum([np.prod(v.get_shape().as_list()) for v in tf.trainable_variables()]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "gan17, epoch  0\n",
      "Generator loss:  1.1679552\n",
      "Discriminator loss:  -0.0044605946\n",
      "\n",
      "gan17, epoch  1\n",
      "Generator loss:  1.1683056\n",
      "Discriminator loss:  -0.0048128366\n",
      "\n",
      "gan17, epoch  2\n",
      "Generator loss:  1.1686541\n",
      "Discriminator loss:  -0.005242512\n",
      "\n",
      "gan17, epoch  3\n",
      "Generator loss:  1.1689895\n",
      "Discriminator loss:  -0.005106319\n",
      "\n",
      "gan17, epoch  4\n",
      "Generator loss:  1.1693397\n",
      "Discriminator loss:  -0.005589882\n",
      "\n",
      "gan17, epoch  5\n",
      "Generator loss:  1.1696727\n",
      "Discriminator loss:  -0.0054268586\n",
      "\n",
      "gan17, epoch  6\n",
      "Generator loss:  1.1700342\n",
      "Discriminator loss:  -0.0068038036\n",
      "\n",
      "gan17, epoch  7\n",
      "Generator loss:  1.1703968\n",
      "Discriminator loss:  -0.007256368\n",
      "\n",
      "gan17, epoch  8\n",
      "Generator loss:  1.1707577\n",
      "Discriminator loss:  -0.007965089\n",
      "\n",
      "gan17, epoch  9\n",
      "Generator loss:  1.171104\n",
      "Discriminator loss:  -0.0074549005\n",
      "\n",
      "gan17, epoch  10\n",
      "Generator loss:  1.1714469\n",
      "Discriminator loss:  -0.007376504\n",
      "\n",
      "gan17, epoch  11\n",
      "Generator loss:  1.1718217\n",
      "Discriminator loss:  -0.008206959\n",
      "\n",
      "gan17, epoch  12\n",
      "Generator loss:  1.1721848\n",
      "Discriminator loss:  -0.0078100385\n",
      "\n",
      "gan17, epoch  13\n",
      "Generator loss:  1.1725683\n",
      "Discriminator loss:  -0.008687474\n",
      "\n",
      "gan17, epoch  14\n",
      "Generator loss:  1.1729074\n",
      "Discriminator loss:  -0.008655094\n",
      "\n",
      "gan17, epoch  15\n",
      "Generator loss:  1.1732814\n",
      "Discriminator loss:  -0.009543726\n",
      "\n",
      "gan17, epoch  16\n",
      "Generator loss:  1.1736547\n",
      "Discriminator loss:  -0.0091061\n",
      "\n",
      "gan17, epoch  17\n",
      "Generator loss:  1.1740324\n",
      "Discriminator loss:  -0.010006075\n",
      "\n",
      "gan17, epoch  18\n",
      "Generator loss:  1.174385\n",
      "Discriminator loss:  -0.010982871\n",
      "\n",
      "gan17, epoch  19\n",
      "Generator loss:  1.1747649\n",
      "Discriminator loss:  -0.010209896\n",
      "\n",
      "gan17, epoch  20\n",
      "Generator loss:  1.1751329\n",
      "Discriminator loss:  -0.010370909\n",
      "\n",
      "gan17, epoch  21\n",
      "Generator loss:  1.1755108\n",
      "Discriminator loss:  -0.0114206625\n",
      "\n",
      "gan17, epoch  22\n",
      "Generator loss:  1.1759056\n",
      "Discriminator loss:  -0.011097353\n",
      "\n",
      "gan17, epoch  23\n",
      "Generator loss:  1.1762702\n",
      "Discriminator loss:  -0.012039666\n",
      "\n",
      "gan17, epoch  24\n",
      "Generator loss:  1.176628\n",
      "Discriminator loss:  -0.01248013\n",
      "\n",
      "gan17, epoch  25\n",
      "Generator loss:  1.1770204\n",
      "Discriminator loss:  -0.013195799\n",
      "\n",
      "gan17, epoch  26\n",
      "Generator loss:  1.1774001\n",
      "Discriminator loss:  -0.012211282\n",
      "\n",
      "gan17, epoch  27\n",
      "Generator loss:  1.1777853\n",
      "Discriminator loss:  -0.01266184\n",
      "\n",
      "gan17, epoch  28\n",
      "Generator loss:  1.1781585\n",
      "Discriminator loss:  -0.013320728\n",
      "\n",
      "gan17, epoch  29\n",
      "Generator loss:  1.1785668\n",
      "Discriminator loss:  -0.013473634\n",
      "\n",
      "gan17, epoch  30\n",
      "Generator loss:  1.1789336\n",
      "Discriminator loss:  -0.013856602\n",
      "\n",
      "gan17, epoch  31\n",
      "Generator loss:  1.1793122\n",
      "Discriminator loss:  -0.013040782\n",
      "\n",
      "gan17, epoch  32\n",
      "Generator loss:  1.1797221\n",
      "Discriminator loss:  -0.014404948\n",
      "\n",
      "gan17, epoch  33\n",
      "Generator loss:  1.1801153\n",
      "Discriminator loss:  -0.014883694\n",
      "\n",
      "gan17, epoch  34\n",
      "Generator loss:  1.1805012\n",
      "Discriminator loss:  -0.015654033\n",
      "\n",
      "gan17, epoch  35\n",
      "Generator loss:  1.1808822\n",
      "Discriminator loss:  -0.014704984\n",
      "\n",
      "gan17, epoch  36\n",
      "Generator loss:  1.1813135\n",
      "Discriminator loss:  -0.015906336\n",
      "\n",
      "gan17, epoch  37\n",
      "Generator loss:  1.1816833\n",
      "Discriminator loss:  -0.016215578\n",
      "\n",
      "gan17, epoch  38\n",
      "Generator loss:  1.1821164\n",
      "Discriminator loss:  -0.016801903\n",
      "\n",
      "gan17, epoch  39\n",
      "Generator loss:  1.1824884\n",
      "Discriminator loss:  -0.017066712\n",
      "\n",
      "gan17, epoch  40\n",
      "Generator loss:  1.18291\n",
      "Discriminator loss:  -0.01680708\n",
      "\n",
      "gan17, epoch  41\n",
      "Generator loss:  1.1833179\n",
      "Discriminator loss:  -0.017006868\n",
      "\n",
      "gan17, epoch  42\n",
      "Generator loss:  1.1836891\n",
      "Discriminator loss:  -0.016694762\n",
      "\n",
      "gan17, epoch  43\n",
      "Generator loss:  1.1841148\n",
      "Discriminator loss:  -0.018646987\n",
      "\n",
      "gan17, epoch  44\n",
      "Generator loss:  1.1844985\n",
      "Discriminator loss:  -0.017781548\n",
      "\n",
      "gan17, epoch  45\n",
      "Generator loss:  1.184916\n",
      "Discriminator loss:  -0.018728416\n",
      "\n",
      "gan17, epoch  46\n",
      "Generator loss:  1.1853282\n",
      "Discriminator loss:  -0.019323818\n",
      "\n",
      "gan17, epoch  47\n",
      "Generator loss:  1.1857477\n",
      "Discriminator loss:  -0.019655392\n",
      "\n",
      "gan17, epoch  48\n",
      "Generator loss:  1.1861633\n",
      "Discriminator loss:  -0.019386383\n",
      "\n",
      "gan17, epoch  49\n",
      "Generator loss:  1.1865834\n",
      "Discriminator loss:  -0.020099169\n",
      "\n",
      "gan17, epoch  50\n",
      "Generator loss:  1.1869769\n",
      "Discriminator loss:  -0.020057207\n",
      "\n",
      "gan17, epoch  51\n",
      "Generator loss:  1.1874067\n",
      "Discriminator loss:  -0.020808835\n",
      "\n",
      "gan17, epoch  52\n",
      "Generator loss:  1.1878451\n",
      "Discriminator loss:  -0.02166061\n",
      "\n",
      "gan17, epoch  53\n",
      "Generator loss:  1.1882446\n",
      "Discriminator loss:  -0.020985708\n",
      "\n",
      "gan17, epoch  54\n",
      "Generator loss:  1.1886455\n",
      "Discriminator loss:  -0.021763973\n",
      "\n",
      "gan17, epoch  55\n",
      "Generator loss:  1.1890903\n",
      "Discriminator loss:  -0.020969223\n",
      "\n",
      "gan17, epoch  56\n",
      "Generator loss:  1.1894988\n",
      "Discriminator loss:  -0.021833912\n",
      "\n",
      "gan17, epoch  57\n",
      "Generator loss:  1.1899358\n",
      "Discriminator loss:  -0.02358562\n",
      "\n",
      "gan17, epoch  58\n",
      "Generator loss:  1.1903707\n",
      "Discriminator loss:  -0.024265783\n",
      "\n",
      "gan17, epoch  59\n",
      "Generator loss:  1.1908056\n",
      "Discriminator loss:  -0.023019252\n",
      "\n",
      "gan17, epoch  60\n",
      "Generator loss:  1.1912293\n",
      "Discriminator loss:  -0.024844717\n",
      "\n",
      "gan17, epoch  61\n",
      "Generator loss:  1.1916391\n",
      "Discriminator loss:  -0.024465794\n",
      "\n",
      "gan17, epoch  62\n",
      "Generator loss:  1.1920879\n",
      "Discriminator loss:  -0.02400654\n",
      "\n",
      "gan17, epoch  63\n",
      "Generator loss:  1.192513\n",
      "Discriminator loss:  -0.025545988\n",
      "\n",
      "gan17, epoch  64\n",
      "Generator loss:  1.1929772\n",
      "Discriminator loss:  -0.025467573\n",
      "\n",
      "gan17, epoch  65\n",
      "Generator loss:  1.1934168\n",
      "Discriminator loss:  -0.027006935\n",
      "\n",
      "gan17, epoch  66\n",
      "Generator loss:  1.1938565\n",
      "Discriminator loss:  -0.026275892\n",
      "\n",
      "gan17, epoch  67\n",
      "Generator loss:  1.1943058\n",
      "Discriminator loss:  -0.02566971\n",
      "\n",
      "gan17, epoch  68\n",
      "Generator loss:  1.1947303\n",
      "Discriminator loss:  -0.028577311\n",
      "\n",
      "gan17, epoch  69\n",
      "Generator loss:  1.1951826\n",
      "Discriminator loss:  -0.027194133\n",
      "\n",
      "gan17, epoch  70\n",
      "Generator loss:  1.1956455\n",
      "Discriminator loss:  -0.027685048\n",
      "\n",
      "gan17, epoch  71\n",
      "Generator loss:  1.1960837\n",
      "Discriminator loss:  -0.02755188\n",
      "\n",
      "gan17, epoch  72\n",
      "Generator loss:  1.1965337\n",
      "Discriminator loss:  -0.029113011\n",
      "\n",
      "gan17, epoch  73\n",
      "Generator loss:  1.196983\n",
      "Discriminator loss:  -0.027729375\n",
      "\n",
      "gan17, epoch  74\n",
      "Generator loss:  1.1974579\n",
      "Discriminator loss:  -0.029445838\n",
      "\n",
      "gan17, epoch  75\n",
      "Generator loss:  1.1978829\n",
      "Discriminator loss:  -0.029616307\n",
      "\n",
      "gan17, epoch  76\n",
      "Generator loss:  1.1983646\n",
      "Discriminator loss:  -0.029331733\n",
      "\n",
      "gan17, epoch  77\n",
      "Generator loss:  1.1988313\n",
      "Discriminator loss:  -0.031859275\n",
      "\n",
      "gan17, epoch  78\n",
      "Generator loss:  1.1992838\n",
      "Discriminator loss:  -0.030319136\n",
      "\n",
      "gan17, epoch  79\n",
      "Generator loss:  1.1997637\n",
      "Discriminator loss:  -0.031673327\n",
      "\n",
      "gan17, epoch  80\n",
      "Generator loss:  1.200232\n",
      "Discriminator loss:  -0.031981062\n",
      "\n",
      "gan17, epoch  81\n",
      "Generator loss:  1.2006692\n",
      "Discriminator loss:  -0.033485413\n",
      "\n",
      "gan17, epoch  82\n",
      "Generator loss:  1.2011572\n",
      "Discriminator loss:  -0.03264183\n",
      "\n",
      "gan17, epoch  83\n",
      "Generator loss:  1.2016338\n",
      "Discriminator loss:  -0.033170875\n",
      "\n",
      "gan17, epoch  84\n",
      "Generator loss:  1.2020992\n",
      "Discriminator loss:  -0.03281348\n",
      "\n",
      "gan17, epoch  85\n",
      "Generator loss:  1.2025574\n",
      "Discriminator loss:  -0.033880904\n",
      "\n",
      "gan17, epoch  86\n",
      "Generator loss:  1.2030511\n",
      "Discriminator loss:  -0.03444833\n",
      "\n",
      "gan17, epoch  87\n",
      "Generator loss:  1.2035162\n",
      "Discriminator loss:  -0.03550731\n",
      "\n",
      "gan17, epoch  88\n",
      "Generator loss:  1.2039968\n",
      "Discriminator loss:  -0.03321094\n",
      "\n",
      "gan17, epoch  89\n",
      "Generator loss:  1.2044908\n",
      "Discriminator loss:  -0.034586597\n",
      "\n",
      "gan17, epoch  90\n",
      "Generator loss:  1.2049385\n",
      "Discriminator loss:  -0.034625772\n",
      "\n",
      "gan17, epoch  91\n",
      "Generator loss:  1.2054406\n",
      "Discriminator loss:  -0.03663816\n",
      "\n",
      "gan17, epoch  92\n",
      "Generator loss:  1.2059346\n",
      "Discriminator loss:  -0.0366853\n",
      "\n",
      "gan17, epoch  93\n",
      "Generator loss:  1.2064232\n",
      "Discriminator loss:  -0.038367063\n",
      "\n",
      "gan17, epoch  94\n",
      "Generator loss:  1.2069097\n",
      "Discriminator loss:  -0.03788041\n",
      "\n",
      "gan17, epoch  95\n",
      "Generator loss:  1.2073847\n",
      "Discriminator loss:  -0.03799903\n",
      "\n",
      "gan17, epoch  96\n",
      "Generator loss:  1.2078881\n",
      "Discriminator loss:  -0.03851919\n",
      "\n",
      "gan17, epoch  97\n",
      "Generator loss:  1.2083851\n",
      "Discriminator loss:  -0.03798238\n",
      "\n",
      "gan17, epoch  98\n",
      "Generator loss:  1.2089057\n",
      "Discriminator loss:  -0.039845575\n",
      "\n",
      "gan17, epoch  99\n",
      "Generator loss:  1.2093908\n",
      "Discriminator loss:  -0.040097937\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    print('\\ngan17, epoch ',epoch)\n",
    "\n",
    "    \n",
    "    # Train discriminator\n",
    "    for i in range(5):\n",
    "        real = np.random.permutation(x_train_flat)[:batch_size].astype(np.float32)\n",
    "        noise_input = np.random.normal(0,1,(batch_size,100))\n",
    "        _,d_loss,grads = sess.run([train_discriminator,diff,grads_discriminator],feed_dict={real_images:real,noise:noise_input})\n",
    "            \n",
    "    # Train generator\n",
    "\n",
    "    real = np.random.permutation(x_train_flat)[:batch_size].astype(np.float32)\n",
    "    noise_input = np.random.normal(0,1,(batch_size,100))\n",
    "    _,g_loss,grads = sess.run([train_generator,gen_loss,grads_generator],feed_dict={noise:noise_input})\n",
    "\n",
    "    print(\"Generator loss: \",g_loss)\n",
    "    print(\"Discriminator loss: \",d_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/ceolson0/Documents/gan_mnist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/ceolson0/Documents/gan_mnist\n"
     ]
    }
   ],
   "source": [
    "saver.restore(sess,'/home/ceolson0/Documents/gan_mnist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.56516594 -0.02657095 -0.51042953 -1.31680594  0.38505696  1.28351455\n",
      " -0.69917019  1.86918227  1.10810286  0.9138034   0.02450456  1.90482537\n",
      "  0.25332172 -0.58592367  0.0795584  -0.95175236  1.0746313  -0.42201506\n",
      " -0.39896597  0.31196309  0.26781133  0.06710148  1.40879167 -0.93542888\n",
      "  0.69427493 -1.92613524 -2.31107768  1.0460165  -0.11278758 -1.06897439\n",
      " -0.18574032  0.0371758   0.49559515 -1.01587094 -0.03022494 -0.78971943\n",
      " -0.34579505 -2.63687331 -0.56602564 -0.92025351 -0.20608061  0.69749193\n",
      "  0.69933888 -0.41545713  0.16903349  0.22999009  0.89393539  1.2333017\n",
      " -0.53503521 -1.18271779 -0.50735141  0.03092944 -1.46801587  0.01128869\n",
      " -0.07096638 -0.44172084 -1.21393603  0.72095396  0.4479347   1.0313349\n",
      " -1.37008016  0.63895701  1.15501191 -0.85037038  1.26730507 -0.77975897\n",
      "  0.66027215 -0.43361723  1.08541481  0.78315488 -1.36773941 -0.56627057\n",
      " -0.401626   -0.73817978  0.70818941 -0.18766593 -1.08938339  1.27048402\n",
      "  0.79027077 -0.15293197  0.22279068 -1.34923026  0.53556001  0.30813297\n",
      "  0.24604863 -0.91703171  1.14028651 -0.83903855  0.06790882 -0.24731523\n",
      " -1.16919218 -0.87516288 -1.53370379 -0.07263845  1.23142977  1.41435032\n",
      "  0.79170617 -0.76054679  2.25800437  0.15636135]\n"
     ]
    }
   ],
   "source": [
    "test_noise = np.random.normal(0,1,(100,100))\n",
    "print(test_noise[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = sess.run(generator(tf.random_normal((batch_size,100)),training=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7ffe394b3748>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "im = test_image[0].reshape((28,28))\n",
    "im = np.stack([im,im,im],axis=-1)\n",
    "from matplotlib import pyplot as plt\n",
    "plt.imshow(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepChem (Python 3.7)",
   "language": "python",
   "name": "deepchem"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
